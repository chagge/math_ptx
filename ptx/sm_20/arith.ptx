//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Thu Jul 31 22:29:38 2014 (1406860178)
// Cuda compilation tools, release 6.5, V6.5.14
//

.version 4.1
.target sm_20
.address_size 64


.visible .func  (.param .b32 func_retval0) ___addff(
	.param .b32 ___addff_param_0,
	.param .b32 ___addff_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___addff_param_0];
	ld.param.f32 	%f2, [___addff_param_1];
	add.f32 	%f3, %f1, %f2;
	st.param.f32	[func_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___adddd(
	.param .b64 ___adddd_param_0,
	.param .b64 ___adddd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___adddd_param_0];
	ld.param.f64 	%fd2, [___adddd_param_1];
	add.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[func_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___addii(
	.param .b32 ___addii_param_0,
	.param .b32 ___addii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___addii_param_0];
	ld.param.u32 	%r2, [___addii_param_1];
	add.s32 	%r3, %r2, %r1;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___adduu(
	.param .b32 ___addjj_param_0,
	.param .b32 ___addjj_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___addjj_param_0];
	ld.param.u32 	%r2, [___addjj_param_1];
	add.s32 	%r3, %r2, %r1;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___addjj(
	.param .b32 ___addcc_param_0,
	.param .b32 ___addcc_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___addcc_param_0];
	ld.param.s8 	%rs2, [___addcc_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	add.s32 	%r3, %r2, %r1;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___addvv(
	.param .b32 ___addhh_param_0,
	.param .b32 ___addhh_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.u8 	%rs1, [___addhh_param_0];
	ld.param.u8 	%rs2, [___addhh_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	add.s32 	%r3, %r2, %r1;
	and.b32  	%r4, %r3, 255;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .align 8 .b8 func_retval0[8]) ___addcc(
	.param .align 8 .b8 ___addcc_param_0[8],
	.param .align 8 .b8 ___addcc_param_1[8]
)
{
	.reg .f32 	%f<7>;


	ld.param.f32 	%f1, [___addcc_param_0+4];
	ld.param.f32 	%f2, [___addcc_param_0];
	ld.param.f32 	%f3, [___addcc_param_1+4];
	ld.param.f32 	%f4, [___addcc_param_1];
	add.f32 	%f5, %f2, %f4;
	add.f32 	%f6, %f1, %f3;
	st.param.f32	[func_retval0+0], %f5;
	st.param.f32	[func_retval0+4], %f6;
	ret;
}

.visible .func  (.param .align 16 .b8 func_retval0[16]) ___addzz(
	.param .align 16 .b8 ___addzz_param_0[16],
	.param .align 16 .b8 ___addzz_param_1[16]
)
{
	.reg .f64 	%fd<7>;


	ld.param.f64 	%fd1, [___addzz_param_0+8];
	ld.param.f64 	%fd2, [___addzz_param_0];
	ld.param.f64 	%fd3, [___addzz_param_1+8];
	ld.param.f64 	%fd4, [___addzz_param_1];
	add.f64 	%fd5, %fd2, %fd4;
	add.f64 	%fd6, %fd1, %fd3;
	st.param.f64	[func_retval0+0], %fd5;
	st.param.f64	[func_retval0+8], %fd6;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___subff(
	.param .b32 ___subff_param_0,
	.param .b32 ___subff_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___subff_param_0];
	ld.param.f32 	%f2, [___subff_param_1];
	sub.f32 	%f3, %f1, %f2;
	st.param.f32	[func_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___subdd(
	.param .b64 ___subdd_param_0,
	.param .b64 ___subdd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___subdd_param_0];
	ld.param.f64 	%fd2, [___subdd_param_1];
	sub.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[func_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___subii(
	.param .b32 ___subii_param_0,
	.param .b32 ___subii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___subii_param_0];
	ld.param.u32 	%r2, [___subii_param_1];
	sub.s32 	%r3, %r1, %r2;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___subuu(
	.param .b32 ___subjj_param_0,
	.param .b32 ___subjj_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___subjj_param_0];
	ld.param.u32 	%r2, [___subjj_param_1];
	sub.s32 	%r3, %r1, %r2;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___subjj(
	.param .b32 ___subcc_param_0,
	.param .b32 ___subcc_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___subcc_param_0];
	ld.param.s8 	%rs2, [___subcc_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	sub.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___subvv(
	.param .b32 ___subhh_param_0,
	.param .b32 ___subhh_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.u8 	%rs1, [___subhh_param_0];
	ld.param.u8 	%rs2, [___subhh_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	sub.s32 	%r3, %r1, %r2;
	and.b32  	%r4, %r3, 255;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .align 8 .b8 func_retval0[8]) ___subcc(
	.param .align 8 .b8 ___subcc_param_0[8],
	.param .align 8 .b8 ___subcc_param_1[8]
)
{
	.reg .f32 	%f<7>;


	ld.param.f32 	%f1, [___subcc_param_0+4];
	ld.param.f32 	%f2, [___subcc_param_0];
	ld.param.f32 	%f3, [___subcc_param_1+4];
	ld.param.f32 	%f4, [___subcc_param_1];
	sub.f32 	%f5, %f2, %f4;
	sub.f32 	%f6, %f1, %f3;
	st.param.f32	[func_retval0+0], %f5;
	st.param.f32	[func_retval0+4], %f6;
	ret;
}

.visible .func  (.param .align 16 .b8 func_retval0[16]) ___subzz(
	.param .align 16 .b8 ___subzz_param_0[16],
	.param .align 16 .b8 ___subzz_param_1[16]
)
{
	.reg .f64 	%fd<7>;


	ld.param.f64 	%fd1, [___subzz_param_0+8];
	ld.param.f64 	%fd2, [___subzz_param_0];
	ld.param.f64 	%fd3, [___subzz_param_1+8];
	ld.param.f64 	%fd4, [___subzz_param_1];
	sub.f64 	%fd5, %fd2, %fd4;
	sub.f64 	%fd6, %fd1, %fd3;
	st.param.f64	[func_retval0+0], %fd5;
	st.param.f64	[func_retval0+8], %fd6;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___mulff(
	.param .b32 ___mulff_param_0,
	.param .b32 ___mulff_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___mulff_param_0];
	ld.param.f32 	%f2, [___mulff_param_1];
	mul.f32 	%f3, %f1, %f2;
	st.param.f32	[func_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___muldd(
	.param .b64 ___muldd_param_0,
	.param .b64 ___muldd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___muldd_param_0];
	ld.param.f64 	%fd2, [___muldd_param_1];
	mul.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[func_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___mulii(
	.param .b32 ___mulii_param_0,
	.param .b32 ___mulii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___mulii_param_0];
	ld.param.u32 	%r2, [___mulii_param_1];
	mul.lo.s32 	%r3, %r2, %r1;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___muluu(
	.param .b32 ___muljj_param_0,
	.param .b32 ___muljj_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___muljj_param_0];
	ld.param.u32 	%r2, [___muljj_param_1];
	mul.lo.s32 	%r3, %r2, %r1;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___muljj(
	.param .b32 ___mulcc_param_0,
	.param .b32 ___mulcc_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___mulcc_param_0];
	ld.param.s8 	%rs2, [___mulcc_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	mul.lo.s32 	%r3, %r2, %r1;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___mulvv(
	.param .b32 ___mulhh_param_0,
	.param .b32 ___mulhh_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.u8 	%rs1, [___mulhh_param_0];
	ld.param.u8 	%rs2, [___mulhh_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	mul.lo.s32 	%r3, %r2, %r1;
	and.b32  	%r4, %r3, 255;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .align 8 .b8 func_retval0[8]) ___mulcc(
	.param .align 8 .b8 ___mulcc_param_0[8],
	.param .align 8 .b8 ___mulcc_param_1[8]
)
{
	.reg .f32 	%f<10>;


	ld.param.f32 	%f1, [___mulcc_param_0+4];
	ld.param.f32 	%f2, [___mulcc_param_0];
	ld.param.f32 	%f3, [___mulcc_param_1+4];
	ld.param.f32 	%f4, [___mulcc_param_1];
	mul.f32 	%f5, %f2, %f4;
	mul.f32 	%f6, %f1, %f3;
	sub.f32 	%f7, %f5, %f6;
	mul.f32 	%f8, %f1, %f4;
	fma.rn.f32 	%f9, %f2, %f3, %f8;
	st.param.f32	[func_retval0+0], %f7;
	st.param.f32	[func_retval0+4], %f9;
	ret;
}

.visible .func  (.param .align 16 .b8 func_retval0[16]) ___mulzz(
	.param .align 16 .b8 ___mulzz_param_0[16],
	.param .align 16 .b8 ___mulzz_param_1[16]
)
{
	.reg .f64 	%fd<10>;


	ld.param.f64 	%fd1, [___mulzz_param_0+8];
	ld.param.f64 	%fd2, [___mulzz_param_0];
	ld.param.f64 	%fd3, [___mulzz_param_1+8];
	ld.param.f64 	%fd4, [___mulzz_param_1];
	mul.f64 	%fd5, %fd2, %fd4;
	mul.f64 	%fd6, %fd1, %fd3;
	sub.f64 	%fd7, %fd5, %fd6;
	mul.f64 	%fd8, %fd1, %fd4;
	fma.rn.f64 	%fd9, %fd2, %fd3, %fd8;
	st.param.f64	[func_retval0+0], %fd7;
	st.param.f64	[func_retval0+8], %fd9;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___divff(
	.param .b32 ___divff_param_0,
	.param .b32 ___divff_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___divff_param_0];
	ld.param.f32 	%f2, [___divff_param_1];
	div.rn.f32 	%f3, %f1, %f2;
	st.param.f32	[func_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___divdd(
	.param .b64 ___divdd_param_0,
	.param .b64 ___divdd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___divdd_param_0];
	ld.param.f64 	%fd2, [___divdd_param_1];
	div.rn.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[func_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___divii(
	.param .b32 ___divii_param_0,
	.param .b32 ___divii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___divii_param_0];
	ld.param.u32 	%r2, [___divii_param_1];
	div.s32 	%r3, %r1, %r2;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___divuu(
	.param .b32 ___divjj_param_0,
	.param .b32 ___divjj_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___divjj_param_0];
	ld.param.u32 	%r2, [___divjj_param_1];
	div.u32 	%r3, %r1, %r2;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___divjj(
	.param .b32 ___divcc_param_0,
	.param .b32 ___divcc_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___divcc_param_0];
	ld.param.s8 	%rs2, [___divcc_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	div.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___divvv(
	.param .b32 ___divhh_param_0,
	.param .b32 ___divhh_param_1
)
{
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___divhh_param_1];
	ld.param.u8 	%rs2, [___divhh_param_0];
	div.u16 	%rs3, %rs2, %rs1;
	cvt.u32.u16	%r1, %rs3;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.visible .func  (.param .align 8 .b8 func_retval0[8]) ___divcc(
	.param .align 8 .b8 ___divcc_param_0[8],
	.param .align 8 .b8 ___divcc_param_1[8]
)
{
	.reg .f32 	%f<23>;


	ld.param.f32 	%f1, [___divcc_param_0+4];
	ld.param.f32 	%f2, [___divcc_param_0];
	ld.param.f32 	%f3, [___divcc_param_1+4];
	ld.param.f32 	%f4, [___divcc_param_1];
	abs.f32 	%f5, %f4;
	abs.f32 	%f6, %f3;
	add.f32 	%f7, %f5, %f6;
	rcp.rn.f32 	%f8, %f7;
	mul.f32 	%f9, %f2, %f8;
	mul.f32 	%f10, %f1, %f8;
	mul.f32 	%f11, %f4, %f8;
	mul.f32 	%f12, %f3, %f8;
	mul.f32 	%f13, %f12, %f12;
	fma.rn.f32 	%f14, %f11, %f11, %f13;
	rcp.rn.f32 	%f15, %f14;
	mul.f32 	%f16, %f10, %f12;
	fma.rn.f32 	%f17, %f9, %f11, %f16;
	mul.f32 	%f18, %f17, %f15;
	mul.f32 	%f19, %f10, %f11;
	mul.f32 	%f20, %f9, %f12;
	sub.f32 	%f21, %f19, %f20;
	mul.f32 	%f22, %f21, %f15;
	st.param.f32	[func_retval0+0], %f18;
	st.param.f32	[func_retval0+4], %f22;
	ret;
}

.visible .func  (.param .align 16 .b8 func_retval0[16]) ___divzz(
	.param .align 16 .b8 ___divzz_param_0[16],
	.param .align 16 .b8 ___divzz_param_1[16]
)
{
	.reg .f64 	%fd<23>;


	ld.param.f64 	%fd1, [___divzz_param_0+8];
	ld.param.f64 	%fd2, [___divzz_param_0];
	ld.param.f64 	%fd3, [___divzz_param_1+8];
	ld.param.f64 	%fd4, [___divzz_param_1];
	abs.f64 	%fd5, %fd4;
	abs.f64 	%fd6, %fd3;
	add.f64 	%fd7, %fd5, %fd6;
	rcp.rn.f64 	%fd8, %fd7;
	mul.f64 	%fd9, %fd2, %fd8;
	mul.f64 	%fd10, %fd1, %fd8;
	mul.f64 	%fd11, %fd4, %fd8;
	mul.f64 	%fd12, %fd3, %fd8;
	mul.f64 	%fd13, %fd12, %fd12;
	fma.rn.f64 	%fd14, %fd11, %fd11, %fd13;
	rcp.rn.f64 	%fd15, %fd14;
	mul.f64 	%fd16, %fd10, %fd12;
	fma.rn.f64 	%fd17, %fd9, %fd11, %fd16;
	mul.f64 	%fd18, %fd17, %fd15;
	mul.f64 	%fd19, %fd10, %fd11;
	mul.f64 	%fd20, %fd9, %fd12;
	sub.f64 	%fd21, %fd19, %fd20;
	mul.f64 	%fd22, %fd21, %fd15;
	st.param.f64	[func_retval0+0], %fd18;
	st.param.f64	[func_retval0+8], %fd22;
	ret;
}


